import json
from typing import List, Tuple, Dict, Optional, Union
from pydantic import Field, BaseModel, ValidationError
from workflow.core.tasks.web_scrapping_tasks.web_scrape_utils import clean_text, fetch_webpage_and_title, preprocess_html, sample_html, extract_json, fallback_parsing_strategy, apply_parsing_strategy
from workflow.core.data_structures import ApiType, References, TaskResponse, References, MessageDict, FunctionParameters, ParameterDefinition, ContentType, URLReference
from workflow.core.tasks.agent_tasks import BasicAgentTask
from workflow.core.data_structures.base_models import TasksEndCodeRouting
from workflow.core.data_structures.task_response_new import NodeResponse
from workflow.core.api import APIManager
from workflow.util import LOGGER

class SelectorModel(BaseModel):
    selectors: List[str]

class WebScrapeBeautifulSoupTask(BasicAgentTask):
    input_variables: FunctionParameters = Field(
        default=FunctionParameters(
            type="object",
            properties={
                "url": ParameterDefinition(
                    type="string",
                    description="The URL of the webpage to scrape."
                ),
                "fetch_url_html_content": ParameterDefinition(
                    type="string",
                    description="The HTML content of the webpage retrieved if you already have it. If you do, URL will be ignored (it is still required).",
                ),
            },
            required=["url"]
        )
    )
    required_apis: List[ApiType] = Field([ApiType.LLM_MODEL], description="A list of required APIs for the task")
    start_node: Optional[str] = Field(default='fetch_url', description="The name of the starting node")
    node_end_code_routing: TasksEndCodeRouting = Field(default={
        'fetch_url':{
            0: {'generate_selectors_and_parse', False},
            1: {'fetch_url', True},
        }, 
        'generate_selectors_and_parse':{
            0: {'None', False},
            1: {'generate_selectors_and', True},
        }
    }, description="A dictionary of tasks/nodes -> exit codes and the task to route to given each exit code")

    async def generate_agent_response(self, api_manager: APIManager, **kwargs) -> Tuple[Optional[References], int, Optional[Union[List[MessageDict], Dict[str, str]]]]:
        url: str = kwargs.get('url', "")
        start_node: str = kwargs.get('start_node', "default")
        if start_node != "default" and start_node != "fetch_url":
            execution_history: List[NodeResponse] = kwargs.get("execution_history", [])
            self_nodes = self.get_self_nodes_from_execution_history(execution_history)
            fetch_node = [node for node in self_nodes if node.node_name == 'fetch_url']
            html_content = fetch_node[-1].references.url_references[-1].content if fetch_node else ""
        page_content: URLReference = await self.retrieve_and_parse_webpage(url, api_manager, start_node=start_node, html_content=html_content)
        if not page_content:
            LOGGER.error("No output returned from API engine.")
            return {}, 1, None
        output = References(url_references=[page_content])
        return output, 0, None
    
    async def run(self, api_manager: APIManager, **kwargs) -> TaskResponse:     
        search_output, exitcode, start_messages = await self.generate_agent_response(api_manager, **kwargs)
        exec_history = kwargs.pop("execution_history", None)
        if not search_output: 
            return self.get_failed_task_response(diagnostics="No messages generated by the agent", **kwargs)
        str_output = search_output.detailed_summary()
        return TaskResponse(
            task_id=self.id,
            task_name=self.task_name,
            task_description=self.task_description,
            status="complete" if exitcode == 0 else "failed",
            result_code=exitcode,
            task_outputs=str_output,
            references=search_output,
            task_inputs=kwargs,
            result_diagnostic="Task executed successfully." if exitcode == 0 else "Task execution failed.",
            execution_history=exec_history
        )
    
    async def retrieve_and_parse_webpage(self, url: str, api_manager: APIManager, start_node: Optional[str], **kwargs) -> URLReference:
        """
        Args:
            url (str): The URL of the webpage to summarize.

        Returns:
            str: The summary of the webpage or an error message.
        """
        LOGGER.info(f"Starting summarization process for URL: {url}")
        try:
            if not start_node or start_node == "default" or start_node == "fetch_url":
                html_content, title = fetch_webpage_and_title(url)
            else:
                html_content = kwargs.get("html_content", None)
                if not html_content:
                    LOGGER.error("No HTML content provided.")
                    return "No HTML content provided."
                title = kwargs.get("title", None)
            cleaned_html = preprocess_html(html_content)
            html_samples = sample_html(cleaned_html)
            selectors, creation_metadata = await self.generate_parsing_instructions(html_samples, api_manager)
            if selectors:
                LOGGER.info(f"Selectors generated by the agent: {selectors}")
                content = apply_parsing_strategy(cleaned_html, selectors)
                if content:
                    return URLReference(title=title, url=url, content=clean_text(content), metadata={"selectors": selectors, "creation_metadata": creation_metadata})
                else:
                    LOGGER.warning("Attempting fallback parsing method.")
            else:
                LOGGER.warning("Failed to generate selectors. Falling back to default parsing.")
                # Fallback to default method: Extract all <p> tags
            try:
                content: str = fallback_parsing_strategy(cleaned_html)
                return URLReference(title=title, url=url, content=clean_text(content), metadata={"selectors": ["p"]})
            except Exception as e:
                LOGGER.error(f"An error occurred while implementing the fallback_parsing_strategy: {e}")
                return "An error occurred while implementing the fallback_parsing_strategy."
        except Exception as e:
            LOGGER.error(f"An error occurred while processing the webpage: {e}")
            return "An error occurred while processing the webpage."

    async def generate_parsing_instructions(self, html_samples: List[str], api_manager: APIManager) -> Tuple[Optional[List[str]], Optional[Dict]]:
        """
        Use an LLM agent to generate CSS selectors in JSON format.

        Args:
            html_samples (List[str]): A list of HTML samples.

        Returns:
            Optional[List[str]]: A list of unique CSS selectors or None if generation fails.
        """
        selectors = []
        creation_metadata = {}
        for idx, sample in enumerate(html_samples, start=1):
            prompt = f"""
    HTML Content:
    {sample}

    Instructions:
    """
            message: MessageDict = MessageDict(role="user", content=prompt, generated_by="tool", type=ContentType.TEXT)
            LOGGER.info(f"Generating selectors for sample {idx}/{len(html_samples)}.")
            try:
                new_messages, start_messages = await self.agent.chat(api_manager=api_manager, messages=[message], max_turns=self.agent.max_consecutive_auto_reply)
                LOGGER.info(f"LLM response: {[msg.model_dump() for msg in new_messages] if new_messages else None}")
                instructions = new_messages[-1].content if new_messages else None
                if new_messages and new_messages[-1].creation_metadata:
                    creation_metadata[f"sample_{idx}"] = new_messages[-1].creation_metadata

                LOGGER.info(f"LLM instructions: {instructions}")
                json_str = extract_json(instructions)
                # Parse the JSON output using Pydantic
                selector_model = SelectorModel.model_validate(json.loads(json_str))
                selectors.extend(selector_model.selectors)
                LOGGER.info(f"Selectors extracted: {selector_model.selectors}")
            except (ValidationError, json.JSONDecodeError) as e:
                LOGGER.warning(f"Invalid JSON received from the LLM for sample {idx}. Error: {e}")
                return None, {}
            except Exception as e:
                LOGGER.error(f"Error during parsing instructions generation for sample {idx}: {e}")
                return None, {}

        # Remove duplicates and maintain order
        unique_selectors = list(dict.fromkeys(selectors))
        LOGGER.info(f"Unique selectors collected: {unique_selectors}")
        return unique_selectors if unique_selectors else None, creation_metadata if creation_metadata else None