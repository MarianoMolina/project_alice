import base64
from typing import List
from pydantic import Field
from openai import AsyncOpenAI
from workflow.core.data_structures import (
    ModelConfig, ApiType, FileContentReference, MessageDict, ContentType, FileType, References, FunctionParameters, ParameterDefinition, RoleTypes, MessageGenerators
    )
from workflow.core.api.engines.api_engine import APIEngine
from workflow.util import LOGGER, chunk_text, get_traceback

class TextToSpeechEngine(APIEngine):
    input_variables: FunctionParameters = Field(
        default=FunctionParameters(
            type="object",
            properties={
                "input": ParameterDefinition(
                    type="string",
                    description="The text to convert to speech."
                ),
                "voice": ParameterDefinition(
                    type="string",
                    description="The voice to use for the speech.",
                    default="alloy"
                ),
                "speed": ParameterDefinition(
                    type="number",
                    description="The speed of the speech.",
                    default=1.0
                ),
            },
            required=["input"]
        )
    )
    required_api: ApiType = Field(ApiType.TEXT_TO_SPEECH, title="The API engine required")

    async def generate_api_response(self, api_data: ModelConfig, input: str, voice: str = "alloy", speed: float = 1.0, **kwargs) -> References:
        """
        Converts text to speech using OpenAI's API and creates a FileContentReference.
        Args:
            api_data (ModelConfig): Configuration data for the API (e.g., API key, base URL).
            input (str): The text to convert to speech.
            model (str): The name of the text-to-speech model to use.
            voice (str): The voice to use for the speech.
            output_filename (Optional[str]): The filename for the generated audio file. If None, a descriptive name will be generated.
        Returns:
            References: A message dict containing information about the generated audio file.
        """
        client = AsyncOpenAI(
            api_key=api_data.api_key,
            base_url=api_data.base_url
        )
        model = api_data.model
        inputs: List[str] = []
        if len(input) > api_data.ctx_size:
            # Calculate the average chunk size
            num_chunks = -(-len(input) // api_data.ctx_size)  # Ceiling division
            avg_chunk_size = len(input) // num_chunks

            # Use the chunk_text utility to split the input
            inputs = chunk_text(input, avg_chunk_size)
        else:
            inputs.append(input)
        responses: List[FileContentReference] = [
            await self.api_call(client, input, voice, speed, model, index) 
            for index, input in enumerate(inputs)
        ]
        return References(files=responses)
    
    async def api_call(self, client: AsyncOpenAI, input: str, voice: str = "alloy", speed: float = 1.0, model: str = None, index: int = 0) -> FileContentReference:
        try:
            LOGGER.debug(f"Generating speech with model {model}, voice {voice}, speed {speed}")
            response = await client.audio.speech.create(
                model=model,
                voice=voice,
                input=input,
                speed=float(speed)
            )
        
            # Get the raw audio data
            audio_data = response.read()
            
            # Generate filename if not provided
            output_filename = self.generate_filename(input, model+voice, index if index != 0 else None, 'mp3')
            
            creation_metadata = {
                    "model": model,
                    "voice": voice,
                    "input_text_length": len(input)
                }
            # Create a FileContentReference
            file_reference = FileContentReference(
                filename=output_filename,
                type=FileType.AUDIO,
                content=base64.b64encode(audio_data).decode('utf-8'),
                transcript=MessageDict(
                    role=RoleTypes.TOOL, 
                    content=f"Speech generated by model {model}. \n\nInput: '{input}' \n\nVoice: {voice}", 
                    type=ContentType.TEXT, 
                    generated_by=MessageGenerators.TOOL, 
                    creation_metadata=creation_metadata)
            )
            return file_reference

        except Exception as e:
            LOGGER.error(f"Error generating speech: {get_traceback()}")
            LOGGER.error(f"Error in OpenAI text-to-speech API call: {str(e)}")
            return References(messages=[MessageDict(
                role=RoleTypes.TOOL, 
                content=f"Error in OpenAI text-to-speech API call: {str(e)}\n\n" + get_traceback(), 
                generated_by=MessageGenerators.SYSTEM)])